{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a688bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for: guitar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ask user for input\n",
    "search_term = input(\"Enter the product you want to search for: \")\n",
    "\n",
    "# format the search term for the Amazon URL\n",
    "formatted_search_term = search_term.replace(\" \", \"+\")\n",
    "\n",
    "# create the URL for the search results page\n",
    "url = f\"https://www.amazon.in/s?k={formatted_search_term}\"\n",
    "\n",
    "# send a request to the search results page\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the page using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the product listings on the page\n",
    "product_listings = soup.find_all(\"div\", {\"class\": \"s-result-item\"})\n",
    "\n",
    "# print the details of each product\n",
    "for product in product_listings:\n",
    "    # find the product title\n",
    "    title_element = product.find(\"h2\", {\"class\": \"a-size-mini\"})\n",
    "    if title_element:\n",
    "        title = title_element.text.strip()\n",
    "    else:\n",
    "        title = \"No title found\"\n",
    "    \n",
    "    # find the product price\n",
    "    price_element = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    if price_element:\n",
    "        price = price_element.text.strip()\n",
    "    else:\n",
    "        price = \"No price found\"\n",
    "    \n",
    "    # print the product details\n",
    "    print(f\"{title} - {price}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335b4b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for: guitar\n",
      "Data for 0 products saved to guitar_products.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# ask user for input\n",
    "search_term = input(\"Enter the product you want to search for: \")\n",
    "\n",
    "# format the search term for the Amazon URL\n",
    "formatted_search_term = search_term.replace(\" \", \"+\")\n",
    "\n",
    "# set the number of pages to scrape\n",
    "num_pages = 3\n",
    "\n",
    "# create an empty list to store the data for each product\n",
    "data = []\n",
    "\n",
    "# scrape data from each page of search results\n",
    "for page in range(1, num_pages+1):\n",
    "    # create the URL for the search results page\n",
    "    url = f\"https://www.amazon.in/s?k={formatted_search_term}&page={page}\"\n",
    "    \n",
    "    # send a request to the search results page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the page using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # find all the product listings on the page\n",
    "    product_listings = soup.find_all(\"div\", {\"class\": \"s-result-item\"})\n",
    "\n",
    "    # loop over each product listing and extract the desired details\n",
    "    for product in product_listings:\n",
    "        # find the product title\n",
    "        title_element = product.find(\"h2\", {\"class\": \"a-size-mini\"})\n",
    "        if title_element:\n",
    "            title = title_element.text.strip()\n",
    "        else:\n",
    "            title = \"-\"\n",
    "        \n",
    "        # find the product brand\n",
    "        brand_element = product.find(\"span\", {\"class\": \"a-size-base-plus\"})\n",
    "        if brand_element:\n",
    "            brand = brand_element.text.strip()\n",
    "        else:\n",
    "            brand = \"-\"\n",
    "        \n",
    "        # find the product price\n",
    "        price_element = product.find(\"span\", {\"class\": \"a-price-whole\"})\n",
    "        if price_element:\n",
    "            price = price_element.text.strip()\n",
    "        else:\n",
    "            price = \"-\"\n",
    "        \n",
    "        # find the product return/exchange info\n",
    "        returns_element = product.find(\"span\", {\"class\": \"a-size-small a-color-secondary\"})\n",
    "        if returns_element:\n",
    "            returns = returns_element.text.strip()\n",
    "        else:\n",
    "            returns = \"-\"\n",
    "        \n",
    "        # find the expected delivery info\n",
    "        delivery_element = product.find(\"span\", {\"class\": \"a-text-bold\"})\n",
    "        if delivery_element:\n",
    "            delivery = delivery_element.text.strip()\n",
    "        else:\n",
    "            delivery = \"-\"\n",
    "        \n",
    "        # find the product availability status\n",
    "        availability_element = product.find(\"div\", {\"class\": \"a-section a-spacing-none\"})\n",
    "        if availability_element:\n",
    "            availability = availability_element.text.strip()\n",
    "        else:\n",
    "            availability = \"-\"\n",
    "        \n",
    "        # find the product URL\n",
    "        url_element = product.find(\"a\", {\"class\": \"a-link-normal a-text-normal\"})\n",
    "        if url_element:\n",
    "            url = \"https://www.amazon.in\" + url_element[\"href\"]\n",
    "        else:\n",
    "            url = \"-\"\n",
    "\n",
    "        # add the product data to the list\n",
    "        data.append([brand, title, price, returns, delivery, availability, url])\n",
    "\n",
    "# create a Pandas DataFrame from the scraped data\n",
    "df = pd.DataFrame(data, columns=[\"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Product URL\"])\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{search_term}_products.csv\", index=False)\n",
    "\n",
    "# print a message to confirm that the data has been saved\n",
    "print(f\"Data for {len(data)} products saved to {search_term}_products.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2bf14e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8064\\1222010471.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# define the keywords to search for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "# define the keywords to search for\n",
    "keywords = [\"fruits\", \"cars\", \"Machine Learning\", \"Guitar\", \"Cakes\"]\n",
    "\n",
    "# set the number of images to scrape for each keyword\n",
    "num_images = 10\n",
    "\n",
    "# create a new Chrome browser instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# loop over each keyword and scrape images\n",
    "for keyword in keywords:\n",
    "    # navigate to the Google Images search page\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "    \n",
    "    # find the search bar and enter the keyword\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "    search_bar.send_keys(keyword)\n",
    "    \n",
    "    # find the search button and click it\n",
    "    search_button = driver.find_element_by_css_selector(\".Tg7LZd\")\n",
    "    search_button.click()\n",
    "    \n",
    "    # scroll down to load more images\n",
    "    for i in range(5):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # get the URLs of the images on the page\n",
    "    image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "    image_urls = [image.get_attribute(\"src\") for image in image_elements]\n",
    "    \n",
    "    # download the images to disk\n",
    "    for i, url in enumerate(image_urls[:num_images]):\n",
    "        filename = f\"{keyword}_{i+1}.jpg\"\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    \n",
    "    print(f\"{num_images} images for '{keyword}' saved to disk.\")\n",
    "\n",
    "# close the browser instance\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6416c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the smartphone to search for: Oneplus Nord\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# specify the URL of the search page for smartphones on Flipkart\n",
    "url = \"https://www.flipkart.com/search?q=\"\n",
    "\n",
    "# ask the user to enter the name of the smartphone to search for\n",
    "search_term = input(\"Enter the name of the smartphone to search for: \")\n",
    "\n",
    "# concatenate the search term with the URL and send a GET request to the page\n",
    "response = requests.get(url + search_term)\n",
    "\n",
    "# parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the div tags containing the search results\n",
    "search_results = soup.find_all(\"div\", {\"class\": \"_2kHMtA\"})\n",
    "\n",
    "# create empty lists to store the scraped data\n",
    "brand_names = []\n",
    "smartphone_names = []\n",
    "colors = []\n",
    "rams = []\n",
    "storages = []\n",
    "primary_cameras = []\n",
    "secondary_cameras = []\n",
    "display_sizes = []\n",
    "battery_capacities = []\n",
    "prices = []\n",
    "product_urls = []\n",
    "\n",
    "# loop over each search result and extract the relevant details\n",
    "for result in search_results:\n",
    "    # extract the brand name and smartphone name\n",
    "    brand_name = result.find(\"div\", {\"class\": \"_4rR01T\"}).text.split()[0]\n",
    "    smartphone_name = result.find(\"div\", {\"class\": \"_4rR01T\"}).text.split()[1:]\n",
    "    smartphone_name = \" \".join(smartphone_name)\n",
    "    \n",
    "    # extract the color\n",
    "    try:\n",
    "        color = result.find(\"div\", {\"class\": \"_3dGepu\"}).text\n",
    "    except:\n",
    "        color = \"-\"\n",
    "    \n",
    "    # extract the RAM and storage\n",
    "    try:\n",
    "        features = result.find_all(\"li\", {\"class\": \"rgWa7D\"})\n",
    "        ram = features[0].text\n",
    "        storage = features[1].text\n",
    "    except:\n",
    "        ram = \"-\"\n",
    "        storage = \"-\"\n",
    "    \n",
    "    # extract the primary and secondary camera\n",
    "    try:\n",
    "        cameras = result.find_all(\"li\", {\"class\": \"rgWa7D\"})[2].text.split(\"|\")\n",
    "        primary_camera = cameras[0]\n",
    "        secondary_camera = cameras[1]\n",
    "    except:\n",
    "        primary_camera = \"-\"\n",
    "        secondary_camera = \"-\"\n",
    "    \n",
    "    # extract the display size\n",
    "    try:\n",
    "        display_size = result.find_all(\"li\", {\"class\": \"rgWa7D\"})[3].text\n",
    "    except:\n",
    "        display_size = \"-\"\n",
    "    \n",
    "    # extract the battery capacity\n",
    "    try:\n",
    "        battery_capacity = result.find_all(\"li\", {\"class\": \"rgWa7D\"})[4].text\n",
    "    except:\n",
    "        battery_capacity = \"-\"\n",
    "    \n",
    "    # extract the price\n",
    "    try:\n",
    "        price = result.find(\"div\", {\"class\": \"_30jeq3 _1_WHN1\"}).text[1:].replace(\",\", \"\")\n",
    "    except:\n",
    "        price = \"-\"\n",
    "    \n",
    "    # extract the product URL\n",
    "    product_url = \"https://www.flipkart.com\" + result.find(\"a\", {\"class\": \"_1fQZEK\"})[\"href\"]\n",
    "    \n",
    "    # append the extracted data to the corresponding lists\n",
    "    brand_names.append(brand_name)\n",
    "    smartphone_names.append(smartphone_name)\n",
    "    colors.append(color)\n",
    "    rams.append(ram)\n",
    "    storages.append(storage)\n",
    "    primary_cameras.append(primary_camera)\n",
    "    secondary_cameras.append(secondary_camera)\n",
    "    display_sizes.append(display_size)\n",
    "    battery_capacities.append(battery_capacity)\n",
    "    prices.append(price)\n",
    "    product_urls.append(product_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156e0c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the city: CHENNAI\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8064\\222482775.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# extract the geospatial coordinates from the page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlatitude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"itemprop\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"latitude\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mlongitude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"itemprop\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"longitude\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ask the user to enter the name of the city to search for\n",
    "city = input(\"Enter the name of the city: \")\n",
    "\n",
    "# construct the URL of the search page by replacing the space in the city name with a plus sign\n",
    "url = f\"https://www.google.com/maps/search/{city.replace(' ', '+')}\"\n",
    "\n",
    "# send a GET request to the search page\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# extract the geospatial coordinates from the page\n",
    "latitude = soup.find(\"meta\", {\"itemprop\": \"latitude\"})[\"content\"]\n",
    "longitude = soup.find(\"meta\", {\"itemprop\": \"longitude\"})[\"content\"]\n",
    "\n",
    "# print the geospatial coordinates\n",
    "print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b46a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# send a GET request to the funding deals page\n",
    "url = \"https://trak.in/india-startup-funding-investment-2015/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find the table containing the funding deals for the second quarter of 2021\n",
    "table = soup.find(\"table\", {\"id\": \"tablepress-48\"})\n",
    "\n",
    "# extract the data from the table and store it in a list of dictionaries\n",
    "data = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    cols = row.find_all(\"td\")\n",
    "    if len(cols) == 0:\n",
    "        continue\n",
    "    date = cols[0].text.strip()\n",
    "    startup = cols[1].text.strip()\n",
    "    industry = cols[2].text.strip()\n",
    "    subvertical = cols[3].text.strip()\n",
    "    city = cols[4].text.strip()\n",
    "    investor = cols[5].text.strip()\n",
    "    investment_type = cols[6].text.strip()\n",
    "    amount = cols[7].text.strip()\n",
    "    data.append({\n",
    "        \"Date\": date,\n",
    "        \"Startup\": startup,\n",
    "        \"Industry\": industry,\n",
    "        \"Subvertical\": subvertical,\n",
    "        \"City\": city,\n",
    "        \"Investor\": investor,\n",
    "        \"InvestmentType\": investment_type,\n",
    "        \"Amount\": amount\n",
    "    })\n",
    "\n",
    "# convert the list of dictionaries to a pandas dataframe and save it to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"funding_deals_q2_2021.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40a38b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8064\\3394684667.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# extract the data for each laptop and store it in a list of dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mlaptop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"right-container\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlaptop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"heading-wraper\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlaptop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"specifications\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# send a GET request to the best gaming laptops page\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find the container containing the list of best gaming laptops\n",
    "container = soup.find(\"div\", {\"class\": \"TopNums\"})\n",
    "\n",
    "# extract the data for each laptop and store it in a list of dictionaries\n",
    "data = []\n",
    "for laptop in container.find_all(\"div\", {\"class\": \"right-container\"}):\n",
    "    name = laptop.find(\"div\", {\"class\": \"heading-wraper\"}).text.strip()\n",
    "    specs = laptop.find(\"div\", {\"class\": \"specifications\"}).text.strip()\n",
    "    price = laptop.find(\"div\", {\"class\": \"price\"}).text.strip().replace(\",\", \"\")\n",
    "    rating = laptop.find(\"div\", {\"class\": \"rating\"}).text.strip()\n",
    "    reviews = laptop.find(\"div\", {\"class\": \"gnumber\"}).text.strip()\n",
    "    image = laptop.find(\"div\", {\"class\": \"product-img\"}).find(\"img\")[\"src\"]\n",
    "    data.append({\n",
    "        \"Name\": name,\n",
    "        \"Specifications\": specs,\n",
    "        \"Price\": price,\n",
    "        \"Rating\": rating,\n",
    "        \"Reviews\": reviews,\n",
    "        \"Image\": image\n",
    "    })\n",
    "\n",
    "# convert the list of dictionaries to a pandas dataframe and save it to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"best_gaming_laptops.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb5153f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8064\\4214605126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# extract the data for each billionaire and store it in a list of dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"td\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# send a GET request to the billionaires page\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the page using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find the table containing the details for all billionaires\n",
    "table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "\n",
    "# extract the data for each billionaire and store it in a list of dictionaries\n",
    "data = []\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    columns = row.find_all(\"td\")\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    net_worth = columns[2].text.strip()\n",
    "    age = columns[3].text.strip()\n",
    "    citizenship = columns[4].text.strip()\n",
    "    source = columns[5].text.strip()\n",
    "    industry = columns[6].text.strip()\n",
    "    data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Name\": name,\n",
    "        \"Net Worth\": net_worth,\n",
    "        \"Age\": age,\n",
    "        \"Citizenship\": citizenship,\n",
    "        \"Source\": source,\n",
    "        \"Industry\": industry\n",
    "    })\n",
    "\n",
    "# convert the list of dictionaries to a pandas dataframe and save it to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"billionaires.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0036d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8064\\3359510057.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moauth2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# replace with your API key file path\n",
    "API_KEY_FILE = \"/path/to/api-key.json\"\n",
    "\n",
    "# replace with the video ID of the video you want to extract comments from\n",
    "VIDEO_ID = \"VIDEO_ID_HERE\"\n",
    "\n",
    "# authenticate using the API key file\n",
    "creds = service_account.Credentials.from_service_account_file(API_KEY_FILE)\n",
    "youtube = build('youtube', 'v3', credentials=creds)\n",
    "\n",
    "# set the earliest time for the comments to be extracted\n",
    "time_delta = timedelta(days=30)\n",
    "now = datetime.utcnow()\n",
    "min_time = (now - time_delta).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# set the maximum number of results to be returned per API request\n",
    "max_results = 100\n",
    "\n",
    "# make API requests to retrieve comments, upvotes, and time of posting\n",
    "comments = []\n",
    "while len(comments) < 500:\n",
    "    response = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=VIDEO_ID,\n",
    "        order='relevance',\n",
    "        textFormat='plainText',\n",
    "        maxResults=max_results,\n",
    "        publishedAfter=min_time\n",
    "    ).execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "        upvotes = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "        time = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "        comments.append({'Comment': comment, 'Upvotes': upvotes, 'Time': time})\n",
    "\n",
    "    if 'nextPageToken' in response:\n",
    "        next_page_token = response['nextPageToken']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# convert the list of comments to a pandas dataframe and save it to a CSV file\n",
    "df = pd.DataFrame(comments)\n",
    "df.to_csv(\"comments.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e7f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "hostels = []\n",
    "\n",
    "for hostel in soup.find_all('div', {'class': 'col-12 col-sm-8 col-md-9 px-sm-1 px-md-2 px-lg-3 search_results pr-md-4'}):\n",
    "    hostel_name = hostel.find('h2', {'class': 'title'}).text.strip()\n",
    "    distance = hostel.find('span', {'class': 'description'}).text.strip()\n",
    "    rating = hostel.find('div', {'class': 'score orange big'}).text.strip()\n",
    "    total_reviews = hostel.find('div', {'class': 'reviews'}).text.strip().replace('Total Reviews', '')\n",
    "    overall_reviews = hostel.find('div', {'class': 'rating'}).text.strip()\n",
    "    price_dorms = hostel.find('div', {'class': 'price'}).text.strip()\n",
    "    price_privates = hostel.find('div', {'class': 'price'}).find_next_sibling('div').text.strip()\n",
    "    facilities = [item.text.strip() for item in hostel.find_all('div', {'class': 'facilities'})]\n",
    "    property_description = hostel.find('div', {'class': 'more'}).text.strip()\n",
    "\n",
    "    hostels.append({\n",
    "        'Hostel Name': hostel_name,\n",
    "        'Distance from City Centre': distance,\n",
    "        'Rating': rating,\n",
    "        'Total Reviews': total_reviews,\n",
    "        'Overall Reviews': overall_reviews,\n",
    "        'Price - Dorms': price_dorms,\n",
    "        'Price - Privates': price_privates,\n",
    "        'Facilities': facilities,\n",
    "        'Property Description': property_description\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(hostels)\n",
    "df.to_csv('london_hostels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bdc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
